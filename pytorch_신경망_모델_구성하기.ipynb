{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pytorch_신경망 모델 구성하기.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNZ63XFwId712xvSt8oIoYw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sim-mi-gyeong/DeepLearning/blob/main/pytorch_%EC%8B%A0%EA%B2%BD%EB%A7%9D_%EB%AA%A8%EB%8D%B8_%EA%B5%AC%EC%84%B1%ED%95%98%EA%B8%B0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P1MpjIB2wOg6"
      },
      "source": [
        "# 신경망: data 연산 수행하는 계층(layer)/모듈(module)로 구성\n",
        "# torch.nn 네임스페이스: 신경망 구성에 필요한 모든 구성요소 제공\n",
        "# Pytorch의 모든 모듈은 nn.Module의 하위 클래스(subclass)\n",
        "# 신경망은 다른 모듈(계층,layer)로 구성된 모듈"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qhl3oaY_EGSy"
      },
      "source": [
        "import os\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ntK39PcEE5zR",
        "outputId": "b6dcc365-c1da-4645-90df-b493ab9e2976"
      },
      "source": [
        "# 학습을 위한 장치 얻기\n",
        "# torch.cuda를 사용할 수 있는지 확인 -> GPU 사용 or CPU 계속 사용\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print('Using {} device'.format(device))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using cpu device\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fWOpoBpJFSGR"
      },
      "source": [
        "# 클래스 정의하기\n",
        "# 신경망 모델을 nn.Module의 하위클래스로 정의\n",
        "# __init__에서 신경망 계층 초기화\n",
        "# nn.Module을 상속받은 모든 class는 forward 메소드에 입력 데이터에 대한 연산 구현 \n",
        "\n",
        "class NeuralNetwork(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(NeuralNetwork, self).__init__()\n",
        "    self.flatten = nn.Flatten()\n",
        "    self.linear_relu_stack = nn.Sequential(\n",
        "        nn.Linear(28*28, 512),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(512, 512),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(512, 10),\n",
        "        nn.ReLU()\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.flatten(x)\n",
        "    logits = self.linear_relu_stack(x)\n",
        "    return logits"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eLQ5p6C0HXo4",
        "outputId": "1690af00-d8a7-4e14-9c91-c826af813eb5"
      },
      "source": [
        "# NeuralNetwork의 인스턴스(instance) 생성 -> device로 이동한 후 -> 구조(structure) 출력\n",
        "\n",
        "model = NeuralNetwork().to(device)\n",
        "print(model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "NeuralNetwork(\n",
            "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
            "  (linear_relu_stack): Sequential(\n",
            "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
            "    (3): ReLU()\n",
            "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
            "    (5): ReLU()\n",
            "  )\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8bus9_jTHttB",
        "outputId": "270d4fc6-bc74-489d-f8e1-bebda8f83d35"
      },
      "source": [
        "# 모델을 사용하기 위해 입력 데이터 전달 -> 일부 백그라운드 연산들과 함께 모델의 forward 실행\n",
        "# model.forward() 직접 호출 X!!\n",
        "\n",
        "# 모델에 입력 호출 -> 각 분류(class)에 대한 원시(raw) 예측값이 있는 10-차원 텐서 반환\n",
        "# 원시 예측값을 nn.Softmax 모듈의 인스턴스에 통과시켜 예측 확률 얻음\n",
        "\n",
        "X = torch.rand(1, 28, 28, device=device)\n",
        "logits = model(X)\n",
        "pred_probab = nn.Softmax(dim=1)(logits)\n",
        "print(f'Predicted Probability : {pred_probab}')\n",
        "y_pred = pred_probab.argmax(1)\n",
        "print(f'Predicted class: {y_pred}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Predicted Probability : tensor([[0.1101, 0.0975, 0.0975, 0.1016, 0.0975, 0.1014, 0.0975, 0.0975, 0.0975,\n",
            "         0.1021]], grad_fn=<SoftmaxBackward>)\n",
            "Predicted class: tensor([0])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4J6FDLJoJMg7",
        "outputId": "9bd93f10-d226-405a-eea7-40047f6383a1"
      },
      "source": [
        "# 모델 계층(Layer)\n",
        "# FashionMNIST 모델의 계층 -> 28x28 크기의 이미지 3개로 구성된 미니배치를 가져와 신경망 통과시 발생하는 일\n",
        "\n",
        "input_image = torch.rand(3, 28, 28)\n",
        "print(input_image.size())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([3, 28, 28])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kqtSsXuvKFq7",
        "outputId": "cf6d687f-6528-47ec-dbd3-b0d57ecc8b94"
      },
      "source": [
        "# nn.Flatten\n",
        "# nn.Flatten 계층을 초기화하여 28x28의 2D 이미지를 784 픽셀 값을 갖는 연속된 배열로 변환\n",
        "# dim=0의 미니배치 차원은 유지\n",
        "\n",
        "flatten = nn.Flatten()\n",
        "flat_image = flatten(input_image)\n",
        "print(flat_image.size())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([3, 784])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GCiYjxxVKiHf",
        "outputId": "92248801-0e3b-41e5-abbb-0e0ad881aaf3"
      },
      "source": [
        "# nn.Linear\n",
        "# 선형 계층은 저장된 가중치(weight)와 편향(bias)을 사용해 입력에 선형 변환 적용하는 모듈\n",
        "\n",
        "layer1 = nn.Linear(in_features = 28*28, out_features = 20)\n",
        "hidden1 = layer1(flat_image)\n",
        "print(hidden1.size())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([3, 20])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xskcVcwVK8sI",
        "outputId": "a2be8272-74a9-4676-ba40-b868b5cda2bb"
      },
      "source": [
        "# nn.ReLU\n",
        "# 비선형 활성화(activation)는 모델의 입력과 출력 사이에 복잡한 관계(mapping) 형성\n",
        "# 선형 변환 후에 적용 -> 비선형성 도입, 신경망이 다양한 현상 학습 할 수 있도록\n",
        "\n",
        "print(f'Before ReLU : {hidden1}\\n\\n')\n",
        "hidden1 = nn.ReLU()(hidden1)\n",
        "print(f'After ReLU : {hidden1}')  # 음수는 0으로"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Before ReLU : tensor([[-0.3244, -0.5843,  0.2743, -0.3958, -0.1436,  0.2623, -0.2406, -0.0793,\n",
            "         -0.2273, -0.2069, -0.2645,  0.0355,  0.1394,  0.3257, -0.5710,  0.3705,\n",
            "         -0.1852,  0.0089, -0.2807,  0.0008],\n",
            "        [-0.1375, -0.5746,  0.6205, -0.3534, -0.2366,  0.6378, -0.0776,  0.1338,\n",
            "          0.0163, -0.0669,  0.1819, -0.4583,  0.5387,  0.4415, -0.7350,  0.2107,\n",
            "         -0.1022, -0.2008, -0.4738, -0.1376],\n",
            "        [-0.1870, -0.2258,  0.1088, -0.0010, -0.3252,  0.1823,  0.0930, -0.1759,\n",
            "          0.0123,  0.0993, -0.0219, -0.3154, -0.0565,  0.2526, -0.4051,  0.4391,\n",
            "          0.2720, -0.1467, -0.7402, -0.0857]], grad_fn=<AddmmBackward>)\n",
            "\n",
            "\n",
            "After ReLU : tensor([[0.0000, 0.0000, 0.2743, 0.0000, 0.0000, 0.2623, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0355, 0.1394, 0.3257, 0.0000, 0.3705, 0.0000, 0.0089,\n",
            "         0.0000, 0.0008],\n",
            "        [0.0000, 0.0000, 0.6205, 0.0000, 0.0000, 0.6378, 0.0000, 0.1338, 0.0163,\n",
            "         0.0000, 0.1819, 0.0000, 0.5387, 0.4415, 0.0000, 0.2107, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000],\n",
            "        [0.0000, 0.0000, 0.1088, 0.0000, 0.0000, 0.1823, 0.0930, 0.0000, 0.0123,\n",
            "         0.0993, 0.0000, 0.0000, 0.0000, 0.2526, 0.0000, 0.4391, 0.2720, 0.0000,\n",
            "         0.0000, 0.0000]], grad_fn=<ReluBackward0>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KE9WeJH6LGW-",
        "outputId": "9b4b0838-fe49-4a21-acd2-51b85908fe8b"
      },
      "source": [
        " # nn.Seauential\n",
        " # 순서를 갖는 모듈의 컨테이너\n",
        " # data는 정의된 순서로 모든 모듈을 통해 전달됨\n",
        "\n",
        " seq_modules = nn.Sequential(\n",
        "     flatten,\n",
        "     layer1,\n",
        "     nn.ReLU(),\n",
        "     nn.Linear(20, 10)\n",
        " )\n",
        " input_image = torch.rand(3, 28, 28)\n",
        " logits = seq_modules(input_image)\n",
        " print(logits)\n",
        " print(logits.size())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[-0.0539,  0.1216, -0.2975,  0.1631,  0.2873,  0.0842,  0.0521,  0.0255,\n",
            "         -0.1463,  0.1449],\n",
            "        [ 0.0287,  0.1107, -0.2793,  0.1868,  0.2885,  0.1934, -0.0455,  0.0629,\n",
            "         -0.1611,  0.0880],\n",
            "        [ 0.0468,  0.0762, -0.3320,  0.2090,  0.3070,  0.1668, -0.0164,  0.0915,\n",
            "         -0.1137,  0.1474]], grad_fn=<AddmmBackward>)\n",
            "torch.Size([3, 10])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ntZ_ZWRUwxMV",
        "outputId": "e8976665-d251-4886-9834-a5201896d5bb"
      },
      "source": [
        "# nn.Softmax\n",
        "# 신경망의 마지막 선형 계층: nn.Softmax 모듈에 전달될 ([-infinty, infinity] 범위의 원시 값(raw value)인) logits 반환\n",
        "# logits: 모델의 각 분류(class)에 대한 예측확률을 나타내도록 [0,1] 범위로 비례하여 조정(scale)됨.\n",
        "# dim 매개변수는 값의 합이 1이 되는 차원을 나타냄\n",
        "\n",
        "softmax = nn.Softmax(dim=1)\n",
        "pred_probab = softmax(logits)\n",
        "\n",
        "print(pred_probab)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[0.0901, 0.1074, 0.0706, 0.1119, 0.1267, 0.1034, 0.1002, 0.0975, 0.0822,\n",
            "         0.1099],\n",
            "        [0.0969, 0.1052, 0.0712, 0.1135, 0.1257, 0.1143, 0.0900, 0.1003, 0.0802,\n",
            "         0.1028],\n",
            "        [0.0975, 0.1004, 0.0668, 0.1147, 0.1265, 0.1099, 0.0915, 0.1019, 0.0830,\n",
            "         0.1078]], grad_fn=<SoftmaxBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7-sN4kDOyMDx",
        "outputId": "ad8cd26e-c76a-4ccc-82df-5050c2efd76d"
      },
      "source": [
        "# 모델 매개변수\n",
        "# 신경망 내부의 계층들은 매개변수화(parameterize)된다. -> 학습 중에 최적화되는 가중치와 편향과 연관지어짐\n",
        "# nn.Module 상속 -> 모델 객체 내부의 모든 필드들이 자동으로 추적/\n",
        "#                   모델의 parameters() 및 named_parameters() 메소드로 모든 매개변수에 접근 가능\n",
        "\n",
        "# 각 매개변수들을 순회(iterate)하며 매개변수의 크기와 값을 출력\n",
        "print(\"Model structure: \", model, \"\\n\\n\")\n",
        "for name, param in model.named_parameters():\n",
        "  print(f\"Layer: {name} | Size: {param.size()} | Values: {param[:2]} \\n\")\n",
        "  print(param)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model structure:  NeuralNetwork(\n",
            "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
            "  (linear_relu_stack): Sequential(\n",
            "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
            "    (3): ReLU()\n",
            "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
            "    (5): ReLU()\n",
            "  )\n",
            ") \n",
            "\n",
            "\n",
            "Layer: linear_relu_stack.0.weight | Size: torch.Size([512, 784]) | Values: tensor([[ 0.0135, -0.0090,  0.0052,  ..., -0.0090, -0.0347,  0.0172],\n",
            "        [ 0.0166,  0.0276,  0.0224,  ...,  0.0078,  0.0200, -0.0235]],\n",
            "       grad_fn=<SliceBackward>) \n",
            "\n",
            "Parameter containing:\n",
            "tensor([[ 0.0135, -0.0090,  0.0052,  ..., -0.0090, -0.0347,  0.0172],\n",
            "        [ 0.0166,  0.0276,  0.0224,  ...,  0.0078,  0.0200, -0.0235],\n",
            "        [-0.0327,  0.0132,  0.0345,  ...,  0.0205,  0.0224,  0.0004],\n",
            "        ...,\n",
            "        [-0.0164,  0.0221, -0.0347,  ..., -0.0269, -0.0081,  0.0336],\n",
            "        [-0.0198, -0.0079, -0.0058,  ..., -0.0146, -0.0019,  0.0240],\n",
            "        [-0.0177, -0.0121,  0.0077,  ..., -0.0114,  0.0013, -0.0060]],\n",
            "       requires_grad=True)\n",
            "Layer: linear_relu_stack.0.bias | Size: torch.Size([512]) | Values: tensor([0.0030, 0.0273], grad_fn=<SliceBackward>) \n",
            "\n",
            "Parameter containing:\n",
            "tensor([ 2.9807e-03,  2.7265e-02,  3.1148e-02, -1.8175e-02, -2.9867e-02,\n",
            "        -1.9301e-02,  3.0292e-03, -2.5769e-02, -1.5071e-02, -2.8244e-05,\n",
            "        -7.0059e-03,  3.4543e-02, -2.4984e-02, -1.8425e-02, -1.8068e-02,\n",
            "         1.9768e-02, -5.6448e-03,  1.9752e-02,  2.8418e-02,  1.4336e-02,\n",
            "         3.3965e-02,  9.2097e-04,  2.6897e-02, -3.3212e-02, -2.8859e-02,\n",
            "        -1.7088e-02, -1.4746e-02,  1.1802e-02,  1.9925e-02, -2.1716e-02,\n",
            "         3.2654e-02,  3.8239e-03, -1.5435e-02, -2.2560e-02,  1.4712e-02,\n",
            "         1.2256e-02, -3.4123e-02, -2.1630e-02, -1.6480e-02,  3.5366e-02,\n",
            "        -3.4792e-02, -5.1411e-03, -3.2304e-02,  5.7070e-03, -8.3049e-03,\n",
            "         2.1954e-02,  2.8628e-02, -1.3366e-02, -1.2585e-02, -9.6990e-03,\n",
            "        -2.5422e-02, -3.0930e-02, -1.1516e-02, -5.9893e-03,  1.8856e-02,\n",
            "         2.2403e-02,  1.0603e-02,  2.1932e-02,  1.4476e-02, -2.3025e-02,\n",
            "        -2.1063e-02, -1.1616e-03,  8.7654e-03, -6.2325e-03,  3.2436e-02,\n",
            "         2.5551e-02, -1.4836e-02, -1.2931e-02, -1.6978e-02,  1.1619e-02,\n",
            "         2.8188e-02,  9.0761e-03,  1.5746e-02,  4.1811e-03,  6.9372e-03,\n",
            "         1.0596e-03, -1.9847e-02,  1.6342e-02,  3.3398e-02,  2.8450e-02,\n",
            "        -2.6152e-02,  1.8184e-02, -5.6641e-04,  7.2392e-03,  3.4463e-02,\n",
            "        -5.0345e-05, -1.8966e-02, -1.9836e-02,  3.0627e-02, -3.5852e-03,\n",
            "         2.3207e-02,  1.8830e-02, -3.3369e-02,  3.3900e-02, -2.3324e-03,\n",
            "        -1.1453e-02, -4.2559e-04,  2.0860e-02, -3.5120e-02, -1.0210e-03,\n",
            "        -1.3722e-05,  5.2712e-03, -6.8686e-03, -2.9985e-02, -4.7105e-03,\n",
            "        -2.8213e-02,  3.1025e-02, -3.4431e-02,  7.3773e-03,  3.3511e-02,\n",
            "         2.5998e-02,  2.7871e-03, -3.2856e-02,  3.5532e-02, -1.4927e-02,\n",
            "         1.5583e-02,  3.0662e-02, -4.7137e-04,  1.8434e-02, -2.9912e-02,\n",
            "         1.3187e-02,  1.7124e-02, -3.5451e-02, -3.0618e-02,  2.7519e-02,\n",
            "         2.3927e-02,  5.6209e-03,  2.7829e-02,  1.2131e-02, -2.9735e-02,\n",
            "         7.3313e-03,  3.5469e-03, -3.5403e-02,  7.6863e-04, -8.6427e-03,\n",
            "         1.6628e-02, -2.0897e-02,  3.8600e-04, -1.7121e-03, -2.6848e-02,\n",
            "         2.9022e-02,  9.8225e-03, -2.9554e-02,  2.0396e-02, -1.9307e-02,\n",
            "        -3.4915e-02, -2.0190e-03,  6.8665e-03, -2.2497e-02,  1.4609e-02,\n",
            "         1.1453e-02, -1.8080e-02,  2.1600e-02, -2.5989e-02, -3.2015e-02,\n",
            "         3.2684e-03, -7.7675e-03, -2.0776e-02,  4.7621e-03, -3.4372e-02,\n",
            "        -5.2174e-03,  2.9426e-02, -3.5803e-03,  1.9526e-02, -3.0933e-02,\n",
            "         1.9952e-02,  3.4503e-02, -2.7713e-02,  4.9535e-03, -2.8722e-02,\n",
            "        -3.0365e-02,  1.5090e-02,  3.3783e-02, -3.5447e-02,  3.4538e-02,\n",
            "        -2.8184e-02, -2.3020e-02, -7.4082e-03, -3.2429e-02,  2.9214e-02,\n",
            "        -2.2932e-02,  1.7929e-02,  2.4966e-02, -3.0492e-03, -1.5432e-02,\n",
            "        -2.3036e-02,  1.6449e-02,  2.1447e-02,  2.9327e-03,  2.4884e-02,\n",
            "        -9.2743e-03, -2.6073e-02, -1.4255e-02,  2.8986e-02,  2.9224e-02,\n",
            "         1.2854e-02, -2.9823e-02,  3.1883e-02,  2.7144e-02, -4.5089e-03,\n",
            "        -1.4375e-04, -2.6032e-02, -1.4227e-02, -1.9658e-02, -8.3475e-03,\n",
            "         7.3838e-03,  2.2068e-02, -2.8960e-02, -3.1911e-02,  9.2706e-03,\n",
            "        -3.0104e-02,  6.4794e-03,  3.0404e-02, -2.0090e-02,  3.4271e-02,\n",
            "         2.7244e-02, -3.0370e-02, -1.8956e-03,  2.0720e-02, -9.1068e-03,\n",
            "        -6.4483e-03,  2.4692e-02,  1.6565e-02, -8.5479e-03, -2.3047e-02,\n",
            "        -2.3329e-03,  2.9524e-02, -1.2807e-02, -2.2930e-02, -4.4655e-04,\n",
            "         5.0406e-03, -2.1289e-02,  9.1322e-04, -2.0459e-02,  2.3887e-02,\n",
            "         8.6875e-03,  9.6941e-03, -1.9725e-02,  2.2075e-02,  3.2398e-02,\n",
            "         2.7580e-02,  4.9460e-03,  9.3630e-03,  1.5348e-02,  3.5752e-03,\n",
            "         3.2167e-02,  1.2723e-02, -2.5246e-02, -3.2111e-02,  7.3469e-03,\n",
            "         1.5656e-03, -1.4344e-02, -1.9662e-02, -2.8941e-02, -3.6014e-03,\n",
            "        -3.2214e-02, -1.7872e-02, -2.9991e-02,  9.0483e-03, -1.3741e-02,\n",
            "        -2.7655e-02,  1.2367e-02, -1.6927e-02, -1.7917e-02,  3.5260e-02,\n",
            "         3.1225e-02, -3.0986e-03,  1.5245e-02, -7.3781e-03,  1.0674e-03,\n",
            "         3.4217e-02,  2.6796e-02,  4.5047e-03, -3.3622e-02, -1.0188e-02,\n",
            "         5.3938e-03,  2.7425e-02,  3.1692e-02, -1.9258e-02, -5.1358e-04,\n",
            "         3.4648e-02, -2.8952e-02, -3.0346e-02,  1.8675e-02, -9.3487e-03,\n",
            "        -2.7696e-02, -2.1604e-02,  6.5937e-03,  6.7797e-03, -2.8220e-02,\n",
            "        -9.1019e-03, -3.3669e-02,  3.2560e-03, -2.7159e-02,  3.2570e-02,\n",
            "         2.6741e-02, -2.8749e-02,  4.9103e-03, -1.9685e-02,  1.0991e-02,\n",
            "        -3.2511e-02, -3.4849e-02, -2.1814e-02,  2.9096e-02,  1.0837e-02,\n",
            "        -3.2216e-02,  2.3025e-02, -2.8092e-03,  7.5146e-03, -3.1243e-02,\n",
            "         1.1534e-02, -1.8496e-02, -2.3882e-03, -2.3223e-02, -2.8473e-02,\n",
            "        -3.9116e-03,  2.9996e-02,  6.0078e-03,  2.1563e-02, -7.9696e-03,\n",
            "         6.8225e-03, -3.4314e-02, -2.8729e-02, -2.8234e-04,  1.0425e-02,\n",
            "         1.4231e-03, -7.8525e-03,  2.7001e-02, -2.9412e-03, -7.4094e-03,\n",
            "        -3.5105e-02,  3.0118e-02, -1.0120e-02, -1.0870e-03, -1.6645e-02,\n",
            "        -1.2932e-03,  2.3357e-02,  1.6134e-02,  2.6034e-02, -5.0375e-03,\n",
            "         4.6749e-03,  4.3888e-03,  1.0052e-02,  1.0933e-02, -1.7626e-02,\n",
            "         2.3639e-02, -1.1464e-02, -2.5346e-03, -4.1260e-03, -3.4498e-02,\n",
            "        -3.5429e-02, -3.1444e-02, -5.6074e-03, -1.4183e-02,  4.9734e-03,\n",
            "         1.9806e-02, -2.1498e-02, -1.3150e-02,  2.7899e-02,  1.4467e-02,\n",
            "        -2.3002e-02, -5.3042e-03, -6.2094e-03, -2.0399e-02,  1.1232e-02,\n",
            "         7.5480e-03,  7.8038e-03, -3.4322e-02, -1.8968e-02,  1.8334e-02,\n",
            "        -3.0246e-03,  1.4700e-02, -3.0640e-02, -1.3339e-02,  6.1276e-03,\n",
            "        -1.2816e-02, -1.9164e-02, -1.7286e-02, -1.7188e-02, -4.5361e-04,\n",
            "        -1.2260e-02, -2.1773e-03,  2.5051e-02,  4.4038e-03, -7.0873e-04,\n",
            "        -1.3627e-02,  1.2129e-02, -3.0529e-02,  1.7014e-02, -6.3035e-03,\n",
            "        -2.5434e-02, -1.3381e-02, -1.2809e-02, -2.5811e-02, -2.9525e-02,\n",
            "        -2.3521e-02,  1.1798e-02,  2.9692e-02, -3.6592e-03, -1.4618e-02,\n",
            "        -2.3352e-02, -1.6665e-02, -1.0327e-02, -3.2004e-03, -2.8920e-02,\n",
            "         1.0793e-02,  1.5086e-02,  1.5539e-02, -1.0261e-02, -1.3269e-02,\n",
            "         2.0763e-02, -1.4029e-02, -3.3813e-02, -1.1097e-02, -1.6076e-03,\n",
            "        -1.6134e-03, -1.4314e-02,  8.5038e-03,  1.2273e-02, -1.4608e-02,\n",
            "         3.3943e-03, -7.1423e-03,  2.6313e-02, -1.3201e-02,  2.5321e-02,\n",
            "        -4.4360e-03, -7.1514e-03,  2.0805e-02, -1.3867e-02,  1.2102e-03,\n",
            "        -2.9326e-02,  1.0388e-02,  8.8383e-03, -1.4185e-02,  1.1968e-02,\n",
            "        -2.3744e-02,  3.2313e-02,  3.0444e-02,  2.5304e-02,  8.8823e-03,\n",
            "        -6.0593e-03, -1.9756e-02, -1.8108e-02,  1.5141e-02, -1.6209e-02,\n",
            "        -1.4964e-02, -2.8852e-02,  2.4444e-02,  2.4518e-02, -1.6548e-02,\n",
            "        -1.7613e-02, -1.7574e-02,  1.5966e-02,  2.4260e-02,  2.7911e-02,\n",
            "         2.3720e-02, -3.1931e-02, -2.2549e-02,  4.9679e-04,  3.2462e-02,\n",
            "        -7.0799e-04,  2.0650e-02,  2.1967e-02, -1.6277e-02, -1.7331e-02,\n",
            "        -1.1647e-03,  1.4933e-02, -1.6142e-02,  2.5820e-02, -2.1272e-02,\n",
            "         1.4870e-02, -8.5171e-03, -4.3049e-03, -2.2199e-02,  2.4040e-02,\n",
            "         3.3508e-02,  2.1404e-02, -6.4001e-03,  2.6230e-02,  3.0321e-02,\n",
            "        -3.5155e-02, -3.1059e-02, -1.9553e-02, -7.5396e-03, -2.6645e-02,\n",
            "         1.5981e-02, -2.5535e-02,  9.5875e-03, -1.7886e-03,  3.0748e-02,\n",
            "         2.6005e-02,  6.7986e-03, -8.6164e-03, -2.9748e-02,  2.8122e-02,\n",
            "        -3.4728e-03, -1.0459e-02,  2.7820e-02,  1.6957e-02,  7.2950e-03,\n",
            "        -3.0914e-03, -7.3528e-03, -2.6531e-02,  2.3654e-02, -3.9268e-03,\n",
            "         1.8533e-02,  2.3437e-02,  2.9300e-02, -1.0223e-02, -1.3644e-02,\n",
            "         2.8888e-02,  2.2015e-02], requires_grad=True)\n",
            "Layer: linear_relu_stack.2.weight | Size: torch.Size([512, 512]) | Values: tensor([[-0.0045,  0.0383, -0.0299,  ...,  0.0421, -0.0069, -0.0195],\n",
            "        [-0.0131,  0.0162,  0.0104,  ...,  0.0007, -0.0437, -0.0343]],\n",
            "       grad_fn=<SliceBackward>) \n",
            "\n",
            "Parameter containing:\n",
            "tensor([[-0.0045,  0.0383, -0.0299,  ...,  0.0421, -0.0069, -0.0195],\n",
            "        [-0.0131,  0.0162,  0.0104,  ...,  0.0007, -0.0437, -0.0343],\n",
            "        [-0.0240, -0.0106, -0.0054,  ...,  0.0367, -0.0197,  0.0211],\n",
            "        ...,\n",
            "        [-0.0432, -0.0344,  0.0164,  ...,  0.0052,  0.0055, -0.0109],\n",
            "        [ 0.0163,  0.0310,  0.0184,  ...,  0.0391, -0.0025, -0.0319],\n",
            "        [ 0.0171, -0.0356,  0.0243,  ...,  0.0239, -0.0322, -0.0207]],\n",
            "       requires_grad=True)\n",
            "Layer: linear_relu_stack.2.bias | Size: torch.Size([512]) | Values: tensor([0.0261, 0.0039], grad_fn=<SliceBackward>) \n",
            "\n",
            "Parameter containing:\n",
            "tensor([ 0.0261,  0.0039, -0.0325,  0.0215, -0.0418,  0.0346,  0.0053,  0.0226,\n",
            "        -0.0377,  0.0159, -0.0421,  0.0047, -0.0018,  0.0297,  0.0283, -0.0140,\n",
            "        -0.0178,  0.0043,  0.0097, -0.0221,  0.0390, -0.0086, -0.0315,  0.0359,\n",
            "         0.0339,  0.0060, -0.0024,  0.0207,  0.0176, -0.0396,  0.0194,  0.0248,\n",
            "        -0.0057, -0.0288,  0.0243, -0.0384, -0.0179, -0.0316,  0.0149,  0.0361,\n",
            "         0.0343,  0.0121, -0.0252, -0.0432,  0.0070, -0.0276, -0.0264, -0.0423,\n",
            "         0.0058,  0.0240,  0.0019,  0.0228, -0.0099,  0.0174,  0.0217, -0.0203,\n",
            "        -0.0414, -0.0002, -0.0311,  0.0437, -0.0392,  0.0225, -0.0381,  0.0435,\n",
            "        -0.0185, -0.0404,  0.0415, -0.0176,  0.0109,  0.0151,  0.0381,  0.0051,\n",
            "        -0.0020, -0.0230,  0.0103, -0.0434, -0.0114, -0.0228, -0.0386, -0.0182,\n",
            "        -0.0317,  0.0034, -0.0050, -0.0005,  0.0188,  0.0068, -0.0110, -0.0266,\n",
            "        -0.0260, -0.0413, -0.0098, -0.0077,  0.0120,  0.0299, -0.0414,  0.0424,\n",
            "         0.0175,  0.0424, -0.0034, -0.0343, -0.0008, -0.0219,  0.0112,  0.0079,\n",
            "        -0.0267, -0.0160, -0.0234, -0.0161, -0.0050,  0.0413, -0.0037,  0.0344,\n",
            "         0.0084, -0.0202, -0.0171, -0.0379, -0.0210,  0.0163,  0.0011, -0.0032,\n",
            "         0.0112, -0.0027,  0.0329, -0.0251,  0.0151,  0.0337, -0.0006, -0.0283,\n",
            "         0.0271,  0.0105,  0.0184,  0.0302,  0.0126,  0.0396, -0.0069, -0.0024,\n",
            "        -0.0268,  0.0355,  0.0148, -0.0307,  0.0022,  0.0044, -0.0105,  0.0166,\n",
            "         0.0418, -0.0161, -0.0145,  0.0369,  0.0196,  0.0028, -0.0292, -0.0192,\n",
            "        -0.0227,  0.0075,  0.0123, -0.0021,  0.0340, -0.0212,  0.0073,  0.0093,\n",
            "         0.0008,  0.0088,  0.0003,  0.0256, -0.0030,  0.0334, -0.0315,  0.0262,\n",
            "        -0.0378,  0.0124, -0.0096, -0.0088,  0.0377, -0.0202, -0.0127, -0.0430,\n",
            "        -0.0328,  0.0258, -0.0225, -0.0086,  0.0422, -0.0061, -0.0063, -0.0296,\n",
            "         0.0111, -0.0266,  0.0326,  0.0087,  0.0046,  0.0015, -0.0180, -0.0365,\n",
            "        -0.0104, -0.0183, -0.0152, -0.0150, -0.0148, -0.0035,  0.0284, -0.0363,\n",
            "        -0.0020, -0.0292, -0.0149, -0.0295,  0.0199, -0.0138, -0.0323,  0.0300,\n",
            "         0.0292,  0.0270, -0.0184,  0.0227, -0.0048, -0.0360, -0.0046, -0.0095,\n",
            "        -0.0298, -0.0148,  0.0060,  0.0339, -0.0121, -0.0441,  0.0188, -0.0233,\n",
            "        -0.0202, -0.0202,  0.0329,  0.0090,  0.0380, -0.0098,  0.0097, -0.0066,\n",
            "         0.0078, -0.0152,  0.0161,  0.0173, -0.0221, -0.0307, -0.0153, -0.0328,\n",
            "         0.0402, -0.0206,  0.0421, -0.0205, -0.0127, -0.0081,  0.0227, -0.0270,\n",
            "         0.0132,  0.0239, -0.0247, -0.0226,  0.0338,  0.0181, -0.0082,  0.0029,\n",
            "         0.0264,  0.0346,  0.0192, -0.0231,  0.0417, -0.0047, -0.0066, -0.0387,\n",
            "        -0.0102, -0.0144, -0.0139,  0.0150,  0.0140,  0.0103,  0.0189, -0.0142,\n",
            "        -0.0348,  0.0251,  0.0174, -0.0387,  0.0234, -0.0160, -0.0085,  0.0341,\n",
            "         0.0132,  0.0386,  0.0406,  0.0265,  0.0029, -0.0056,  0.0256, -0.0190,\n",
            "        -0.0236,  0.0434, -0.0252,  0.0271,  0.0223,  0.0074, -0.0137,  0.0222,\n",
            "         0.0283, -0.0347, -0.0048,  0.0276, -0.0236,  0.0316, -0.0198,  0.0253,\n",
            "        -0.0393,  0.0379, -0.0089, -0.0311,  0.0135,  0.0122, -0.0335,  0.0285,\n",
            "         0.0416,  0.0009, -0.0092, -0.0011,  0.0011,  0.0165,  0.0314,  0.0075,\n",
            "        -0.0045,  0.0416, -0.0351, -0.0198, -0.0363, -0.0423,  0.0018,  0.0062,\n",
            "        -0.0076, -0.0276, -0.0321, -0.0124,  0.0371,  0.0298, -0.0177, -0.0191,\n",
            "         0.0424,  0.0147,  0.0118,  0.0148,  0.0165, -0.0227,  0.0146,  0.0262,\n",
            "         0.0231,  0.0440, -0.0069, -0.0144, -0.0013,  0.0411,  0.0288,  0.0304,\n",
            "        -0.0318, -0.0362, -0.0005,  0.0046, -0.0035, -0.0133,  0.0269,  0.0039,\n",
            "         0.0433,  0.0301,  0.0413,  0.0291, -0.0383, -0.0431,  0.0037, -0.0359,\n",
            "        -0.0145,  0.0227, -0.0346,  0.0108, -0.0001, -0.0342,  0.0098, -0.0219,\n",
            "         0.0406, -0.0298,  0.0114, -0.0220,  0.0131, -0.0162,  0.0381, -0.0101,\n",
            "         0.0132, -0.0040,  0.0013,  0.0112,  0.0107,  0.0047,  0.0399,  0.0324,\n",
            "         0.0426, -0.0079,  0.0381,  0.0428, -0.0409, -0.0432,  0.0422, -0.0385,\n",
            "         0.0283,  0.0048, -0.0330, -0.0148, -0.0052,  0.0004, -0.0238, -0.0429,\n",
            "         0.0225, -0.0380,  0.0073,  0.0223, -0.0332,  0.0139, -0.0170,  0.0209,\n",
            "        -0.0410,  0.0389, -0.0298,  0.0384, -0.0354,  0.0170, -0.0040, -0.0248,\n",
            "        -0.0221, -0.0201, -0.0065,  0.0212,  0.0248,  0.0308,  0.0426,  0.0283,\n",
            "        -0.0195,  0.0396, -0.0435, -0.0335,  0.0353, -0.0328,  0.0197,  0.0232,\n",
            "        -0.0300, -0.0278,  0.0195,  0.0227, -0.0426,  0.0297,  0.0394,  0.0343,\n",
            "         0.0411, -0.0426,  0.0109,  0.0397, -0.0085, -0.0413, -0.0005,  0.0082,\n",
            "         0.0071, -0.0271,  0.0190,  0.0023, -0.0181,  0.0352,  0.0285, -0.0421,\n",
            "        -0.0207, -0.0423,  0.0300, -0.0196,  0.0434,  0.0427,  0.0291, -0.0408,\n",
            "         0.0282,  0.0387,  0.0093, -0.0142,  0.0104, -0.0184, -0.0109,  0.0067,\n",
            "        -0.0432, -0.0170,  0.0109, -0.0232,  0.0329, -0.0300,  0.0282, -0.0388,\n",
            "        -0.0142, -0.0347,  0.0322, -0.0234,  0.0007,  0.0122,  0.0305, -0.0360,\n",
            "        -0.0224, -0.0049,  0.0179, -0.0238,  0.0179, -0.0337, -0.0106,  0.0244,\n",
            "        -0.0266, -0.0308,  0.0087, -0.0387,  0.0269, -0.0232, -0.0263, -0.0133],\n",
            "       requires_grad=True)\n",
            "Layer: linear_relu_stack.4.weight | Size: torch.Size([10, 512]) | Values: tensor([[ 0.0370, -0.0436, -0.0074,  ...,  0.0418,  0.0364, -0.0085],\n",
            "        [ 0.0321, -0.0339, -0.0343,  ..., -0.0024,  0.0408, -0.0168]],\n",
            "       grad_fn=<SliceBackward>) \n",
            "\n",
            "Parameter containing:\n",
            "tensor([[ 0.0370, -0.0436, -0.0074,  ...,  0.0418,  0.0364, -0.0085],\n",
            "        [ 0.0321, -0.0339, -0.0343,  ..., -0.0024,  0.0408, -0.0168],\n",
            "        [ 0.0106, -0.0315, -0.0162,  ...,  0.0154,  0.0169,  0.0404],\n",
            "        ...,\n",
            "        [ 0.0306, -0.0176, -0.0210,  ..., -0.0264,  0.0115, -0.0411],\n",
            "        [ 0.0263,  0.0293,  0.0283,  ...,  0.0051, -0.0088, -0.0317],\n",
            "        [-0.0105,  0.0376,  0.0395,  ...,  0.0016, -0.0371,  0.0074]],\n",
            "       requires_grad=True)\n",
            "Layer: linear_relu_stack.4.bias | Size: torch.Size([10]) | Values: tensor([-0.0143, -0.0072], grad_fn=<SliceBackward>) \n",
            "\n",
            "Parameter containing:\n",
            "tensor([-0.0143, -0.0072,  0.0034,  0.0164, -0.0286,  0.0253, -0.0082,  0.0145,\n",
            "         0.0228,  0.0205], requires_grad=True)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "chHtX4GVm_n_"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}